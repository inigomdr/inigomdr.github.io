<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
.collapsible {
  background-color: #0076C2;
  color: white;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: none;
  font-family: 'Helvetica', 'Arial', sans-serif;
  text-align: left;
  outline: none;
  font-size: 15px;
}

.active, .collapsible:hover {
  background-color: #00A6D6;
}

.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
  background-color: #f1f1f1;
}
</style>
</head>
<body>

<h1>Sociotechnical AI Systems Lab</h1>

  <p>SAIL does <b>this</b> & <b>that</b></p>
  
<h3>People</h3>
  <button type="button" class="collapsible">Roel Dobbe</button>
    <div class="content">
      <p>Bio</p>
    </div>
  <button type="button" class="collapsible">Sem Nouws</button>
    <div class="content">
      <p>Bio</p>
    </div>
  <button type="button" class="collapsible">Íñigo MDR de Troya</button>
    <div class="content">
      <p>
        Íñigo is a PhD researcher studying how AI system safety is negotiated and realised in efforts for addressing and preventing sociotechnical harms in public services.
        Through empirical and conceptual work, he is developing an understanding of how to situate AI in a broader sociotechnical context.
        Íñigo's PhD is funded by the Hybrid Intelligence gravitation/zwaartekracht program of the Dutch Research Council (NWO), and is supervised by Profs. Roel Dobbe, Neelke Doorn, and Virginia Dignum.
        Prior to TU Delft, Íñigo was a data scientist and research fellow at the Data Science for Social Good Foundation, based at Nova SBE in Lisbon, Portugal.
        Before turning to public policy, he worked on machine listening applications for home security and birdwatching.
        Íñigo received an MEng in Electrical & Electronic Engineering from Imperial College London, and an MRes in Neuroinformatics & Computational Neuroscience at The University of Edinburgh.
      </p>
    </div>
  <button type="button" class="collapsible">Eva de Winkel</button>
    <div class="content">
      <p>
        Eva is a PhD researcher focusing on in the integration of fairness and justice into algorithmic control systems, particularly for addressing grid congestion in electrical distribution grids. 
        Her research considers these challenges from a system's perspective, recognizing that the increasing complexity of the electricity grid and the broader energy system requires a holistic approach. 
        This PhD project is part of the AI for Energy Grids lab, in collaboration with Alliander, one of the distribution system operators in the Netherlands. 
        Eva holds a bachelor’s and master’s degree in aerospace engineering from Delft University of Technology.
        Following her graduation, she gained industry experience as a project engineer in the contracting and construction of wind energy projects. 
      </p>
    </div>
  <button type="button" class="collapsible">Jacqueline Kernahan</button>
    <div class="content">
      <p>Bio</p>
    </div>

<h3>Publications</h3>
  
  <button type="button" class="collapsible">The Rule of Law for Artificial Intelligence in Public Administration: A System Safety Perspective</button>
    <div class="content">
      <p>
        Nouws, S. J. J., & Dobbe, R. I. J. (2024). The Rule of Law for Artificial Intelligence in Public Administration: A System Safety Perspective. In Digital Governance: confronting the challenges posed by Artificial Intelligence. TMC Asser Press.
        <br><br>
        This chapter proposes an analytical lens to comprehensively address the role of Artificial Intelligence (AI) applications in mediating arbitrary exercise of power in public administration and the citizen harms that result from such conduct. It provides a timely and urgent account to fill gaps in conventional Rule of Law thought. AI systems are socio-technical by nature and, therefore, differ from the text-driven social constructs that the legal professions dealing with Rule of Law issues concentrate on. Put to work in public administration contexts with consequential decision-making, technical artefacts can contribute to a variety of hazardous situations that provide opportunities for arbitrary conduct. A comprehensive lens to understand and address the role of technology in Rule of Law violations has largely been missing in literature. We propose to combine a socio-legal perspective on the Rule of Law with central insights from system safety–a safety engineering tradition with a strong scientific as well as real-world practice–that considers safety from a technological, systemic, and institutional perspective. The combination results in a lexicon and analytical approach that enables public organisations to identify possibilities for arbitrary conduct in public AI systems. Following on the analysis, interventions can be designed to prevent, mitigate, or correct system hazards and, thereby, protect citizens against arbitrary exercise of power.
      </p>
    </div>

  <button type="button" class="collapsible">AI Alignment through Reinforcement Learning from Human Feedback? Contradictions and Limitations</button>
      <div class="content">
        <p>
          Lindström, A. D., Methnani, L., Krause, L., Ericson, P., de Troya, Í. M. D. R., Mollo, D. C., & Dobbe, R. (2024). AI Alignment through Reinforcement Learning from Human Feedback? Contradictions and Limitations. arXiv preprint arXiv:2406.18346
          <br><br>
          This paper critically evaluates the attempts to align Artificial Intelligence (AI) systems, especially Large Language Models (LLMs), with human values and intentions through Reinforcement Learning from Feedback (RLxF) methods, involving either human feedback (RLHF) or AI feedback (RLAIF). Specifically, we show the shortcomings of the broadly pursued alignment goals of honesty, harmlessness, and helpfulness. Through a multidisciplinary sociotechnical critique, we examine both the theoretical underpinnings and practical implementations of RLxF techniques, revealing significant limitations in their approach to capturing the complexities of human ethics and contributing to AI safety. We highlight tensions and contradictions inherent in the goals of RLxF. In addition, we discuss ethically-relevant issues that tend to be neglected in discussions about alignment and RLxF, among which the trade-offs between user-friendliness and deception, flexibility and interpretability, and system safety. We conclude by urging researchers and practitioners alike to critically assess the sociotechnical ramifications of RLxF, advocating for a more nuanced and reflective approach to its application in AI development.
        </p>
      </div>

  <button type="button" class="collapsible">System Safety & Artificial Intelligence</button>
    <div class="content">
      <p>
        Roel Dobbe. 2022. System Safety and Artificial Intelligence. In The Oxford Handbook of AI Governance. Oxford University Press. https://doi.org/10.1093/oxfordhb/9780197579329.001.0001
        <br><br>
        This chapter formulates seven lessons for preventing harm in artificial intelligence (AI) systems based on insights from the field of system safety for software-based automation in safety-critical domains. New applications of AI across societal domains and public organizations and infrastructures come with new hazards, which lead to new forms of harm, both grave and pernicious. The chapter addresses the lack of consensus for diagnosing and eliminating new AI system hazards. For decades, the field of system safety has dealt with accidents and harm in safety-critical systems governed by varying degrees of software-based automation and decision-making. This field embraces the core assumption of systems and control that AI systems cannot be safeguarded by technical design choices on the model or algorithm alone, instead requiring an end-to-end hazard analysis and design frame that includes the context of use, impacted stakeholders, and the formal and informal institutional environment in which the system operates. Safety and other values are then inherently socio-technical and emergent system properties that require design and control measures to instantiate these across the technical, social, and institutional components of a system. This chapter honors system safety pioneer Nancy Leveson, by situating her core lessons for today’s AI system safety challenges. For every lesson, concrete tools are offered for rethinking and reorganizing the safety management of AI systems, both in design and governance. This history tells us that effective AI safety management requires transdisciplinary approaches and a shared language that allows involvement of all levels of society.
      </p>
    </div>

<h3>Get in touch</h3>
  <p>Like this & like that.</p>

<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>

</body>
</html>
