<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>

* {
  font-family: 'Helvetica', 'Arial', sans-serif;
}
  
.collapsible {
  background-color: #0076C2;
  color: white;
  cursor: pointer;
  padding: 18px;
  width: 50%;
  border: none;
  font-family: 'Helvetica', 'Arial', sans-serif;
  text-align: left;
  outline: none;
  font-size: 15px;
}

.active, .collapsible:hover {
  background-color: #00A6D6;
}

.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
  background-color: #f1f1f1;
}
</style>
</head>
<body>

<h1>Sociotechnical AI Systems Lab</h1>

  <p>
    The Sociotechnical AI Systems Lab works to develop principles and practices for the integral analysis, design and governance of AI and algorithmic tools in their sociotechnical system context. <br>
    <br>
    Our work is situated in different domains, including social security, energy systems and healthcare, and engages with actors that aim to understand and promote safety, justice and sustainability.<br>      
  </p>
  
<h3>People</h3>
  
  <button type="button" class="collapsible">Roel Dobbe</button>
    <div class="content">
      <p>Bio</p>
    </div>
  
  <button type="button" class="collapsible">Sem Nouws</button>
    <div class="content">
      <p>
        Sem Nouws is a PhD researcher studying how the socio-technical design process of public algorithmic systems can be embedded in democratic and Rule of Law practices. 
        He is formulating institutional interventions that can help public organisations to transform their design processes. 
        Sem holds a master degree Complex Systems Engineering and Management from the TU Delft and a bachelor degree in Law from the Erasmus University.
        Before his PhD, Sem joined the <i>Nationale Denktank</i> where he worked on a policy proposal that focuses on the ethical aspects surrounding the collection of data from individual internet users.
      </p>
    </div>
  
  <button type="button" class="collapsible">Íñigo de Troya</button>
    <div class="content">
      <p>
        Íñigo is a PhD researcher studying how AI system safety is negotiated and realised in efforts for addressing and preventing sociotechnical harms in public services.
        Through empirical and conceptual work, he is developing an understanding of how to situate AI in a broader sociotechnical context.
        Íñigo's PhD is funded by the Hybrid Intelligence gravitation/zwaartekracht program of the Dutch Research Council (NWO), and is supervised by Profs. Roel Dobbe, Neelke Doorn, and Virginia Dignum.
        Prior to TU Delft, Íñigo was a data scientist and research fellow at the Data Science for Social Good Foundation, based at Nova SBE in Lisbon, Portugal.
        Before turning to public policy, he worked on machine listening applications for home security and birdwatching.
        Íñigo received an MEng in Electrical & Electronic Engineering from Imperial College London, and an MRes in Neuroinformatics & Computational Neuroscience at The University of Edinburgh.
      </p>
    </div>
  
  <button type="button" class="collapsible">Eva de Winkel</button>
    <div class="content">
      <p>
        Eva is a PhD researcher focusing on in the integration of fairness and justice into algorithmic control systems, particularly for addressing grid congestion in electrical distribution grids. 
        Her research considers these challenges from a system's perspective, recognizing that the increasing complexity of the electricity grid and the broader energy system requires a holistic approach. 
        This PhD project is part of the AI for Energy Grids lab, in collaboration with Alliander, one of the distribution system operators in the Netherlands. 
        Eva holds a bachelor’s and master’s degree in aerospace engineering from Delft University of Technology.
        Following her graduation, she gained industry experience as a project engineer in the contracting and construction of wind energy projects. 
      </p>
    </div>
  
  <button type="button" class="collapsible">Jacqueline Kernahan</button>
    <div class="content">
      <p>Bio</p>
    </div>

  <button type="button" class="collapsible">Siddharth Mehrotra</button>
    <div class="content">
      <p>Bio</p>
    </div>

  <button type="button" class="collapsible">Jin Huang</button>
    <div class="content">
      <p>Bio</p>
    </div>

<h3>Publications</h3>

  <button type="button" class="collapsible">AI System Safety</button>
    <div class="content">
  
    <button type="button" class="collapsible">The Rule of Law for Artificial Intelligence in Public Administration: A System Safety Perspective</button>
      <div class="content">
        <p>
          <b>Abstract</b><br>
          This chapter proposes an analytical lens to comprehensively address the role of Artificial Intelligence (AI) applications in mediating arbitrary exercise of power in public administration and the citizen harms that result from such conduct. It provides a timely and urgent account to fill gaps in conventional Rule of Law thought. AI systems are socio-technical by nature and, therefore, differ from the text-driven social constructs that the legal professions dealing with Rule of Law issues concentrate on. Put to work in public administration contexts with consequential decision-making, technical artefacts can contribute to a variety of hazardous situations that provide opportunities for arbitrary conduct. A comprehensive lens to understand and address the role of technology in Rule of Law violations has largely been missing in literature. We propose to combine a socio-legal perspective on the Rule of Law with central insights from system safety–a safety engineering tradition with a strong scientific as well as real-world practice–that considers safety from a technological, systemic, and institutional perspective. The combination results in a lexicon and analytical approach that enables public organisations to identify possibilities for arbitrary conduct in public AI systems. Following on the analysis, interventions can be designed to prevent, mitigate, or correct system hazards and, thereby, protect citizens against arbitrary exercise of power.
          <br><br>
          Nouws, S. J. J., & Dobbe, R. I. J. (2024). The Rule of Law for Artificial Intelligence in Public Administration: A System Safety Perspective. In Digital Governance: confronting the challenges posed by Artificial Intelligence. TMC Asser Press.
        </p>
      </div>
  
    <button type="button" class="collapsible">AI Alignment through Reinforcement Learning from Human Feedback? Contradictions and Limitations</button>
        <div class="content">
          <p>
            <b>Abstract</b><br>
            This paper critically evaluates the attempts to align Artificial Intelligence (AI) systems, especially Large Language Models (LLMs), with human values and intentions through Reinforcement Learning from Feedback (RLxF) methods, involving either human feedback (RLHF) or AI feedback (RLAIF). Specifically, we show the shortcomings of the broadly pursued alignment goals of honesty, harmlessness, and helpfulness. Through a multidisciplinary sociotechnical critique, we examine both the theoretical underpinnings and practical implementations of RLxF techniques, revealing significant limitations in their approach to capturing the complexities of human ethics and contributing to AI safety. We highlight tensions and contradictions inherent in the goals of RLxF. In addition, we discuss ethically-relevant issues that tend to be neglected in discussions about alignment and RLxF, among which the trade-offs between user-friendliness and deception, flexibility and interpretability, and system safety. We conclude by urging researchers and practitioners alike to critically assess the sociotechnical ramifications of RLxF, advocating for a more nuanced and reflective approach to its application in AI development.
            <br><br>
            Lindström, A. D., Methnani, L., Krause, L., Ericson, P., de Troya, Í. M. D. R., Mollo, D. C., & Dobbe, R. (2024). AI Alignment through Reinforcement Learning from Human Feedback? Contradictions and Limitations. <a href="https://arxiv.org/abs/2406.18346">arXiv preprint arXiv:2406.18346</a>
          </p>
        </div>
  
    <button type="button" class="collapsible">System Safety & Artificial Intelligence</button>
      <div class="content">
        <p>
          <b>Abstract</b><br>
          This chapter formulates seven lessons for preventing harm in artificial intelligence (AI) systems based on insights from the field of system safety for software-based automation in safety-critical domains. New applications of AI across societal domains and public organizations and infrastructures come with new hazards, which lead to new forms of harm, both grave and pernicious. The chapter addresses the lack of consensus for diagnosing and eliminating new AI system hazards. For decades, the field of system safety has dealt with accidents and harm in safety-critical systems governed by varying degrees of software-based automation and decision-making. This field embraces the core assumption of systems and control that AI systems cannot be safeguarded by technical design choices on the model or algorithm alone, instead requiring an end-to-end hazard analysis and design frame that includes the context of use, impacted stakeholders, and the formal and informal institutional environment in which the system operates. Safety and other values are then inherently socio-technical and emergent system properties that require design and control measures to instantiate these across the technical, social, and institutional components of a system. This chapter honors system safety pioneer Nancy Leveson, by situating her core lessons for today’s AI system safety challenges. For every lesson, concrete tools are offered for rethinking and reorganizing the safety management of AI systems, both in design and governance. This history tells us that effective AI safety management requires transdisciplinary approaches and a shared language that allows involvement of all levels of society.
          <br><br>
          Roel Dobbe. 2022. System Safety and Artificial Intelligence. In The Oxford Handbook of AI Governance. Oxford University Press. <a href="https://doi.org/10.1093/oxfordhb/9780197579329.001.0001">https://doi.org/10.1093/oxfordhb/9780197579329.001.0001</a>
        </p>
      </div>

    </div>


  <button type="button" class="collapsible">Generative AI</button>
    <div class="content">
  
    <button type="button" class="collapsible">AI Alignment through Reinforcement Learning from Human Feedback? Contradictions and Limitations</button>
        <div class="content">
          <p>
            <b>Abstract</b><br>
            This paper critically evaluates the attempts to align Artificial Intelligence (AI) systems, especially Large Language Models (LLMs), with human values and intentions through Reinforcement Learning from Feedback (RLxF) methods, involving either human feedback (RLHF) or AI feedback (RLAIF). Specifically, we show the shortcomings of the broadly pursued alignment goals of honesty, harmlessness, and helpfulness. Through a multidisciplinary sociotechnical critique, we examine both the theoretical underpinnings and practical implementations of RLxF techniques, revealing significant limitations in their approach to capturing the complexities of human ethics and contributing to AI safety. We highlight tensions and contradictions inherent in the goals of RLxF. In addition, we discuss ethically-relevant issues that tend to be neglected in discussions about alignment and RLxF, among which the trade-offs between user-friendliness and deception, flexibility and interpretability, and system safety. We conclude by urging researchers and practitioners alike to critically assess the sociotechnical ramifications of RLxF, advocating for a more nuanced and reflective approach to its application in AI development.
            <br><br>
            Lindström, A. D., Methnani, L., Krause, L., Ericson, P., de Troya, Í. M. D. R., Mollo, D. C., & Dobbe, R. (2024). AI Alignment through Reinforcement Learning from Human Feedback? Contradictions and Limitations. <a href="https://arxiv.org/abs/2406.18346">arXiv preprint arXiv:2406.18346</a>
          </p>
        </div>

    </div>


  <button type="button" class="collapsible">Healthcare</button>
    <div class="content">
  
    <button type="button" class="collapsible">Hybrid Intelligence Supports Application Development for Diabetes Lifestyle Management</button>
        <div class="content">
          <p>
            <b>Abstract</b><br>
            Type II diabetes is a complex health condition requiring patients to closely and continuously collaborate with healthcare professionals and other caretakers on lifestyle changes. While intelligent products have tremendous potential to support such Diabetes Lifestyle Management (DLM), existing products are typically conceived from a technology-centered perspective that insufficiently acknowledges the degree to which collaboration and inclusion of stakeholders is required. In this article, we argue that the emergent design philosophy of Hybrid Intelligence (HI) forms a suitable alternative lens for research and development. In particular, we (1) highlight a series of pragmatic challenges for effective AI-based DLM support based on results from an expert focus group, and (2) argue for HI’s potential to address these by outlining relevant research trajectories.
            <br><br>
            Dudzik, B. J., van der Waa, J. S., Chen, P. Y., Dobbe, R., de Troya, Í. M., Bakker, R. M., ... & Kamphorst, B. A. (2024). Hybrid Intelligence Supports Application Development for Diabetes Lifestyle Management. Journal of Artificial Intelligence Research, 80, 919-929. <a href="https://doi.org/10.1613/jair.1.15916">https://doi.org/10.1613/jair.1.15916</a>
          </p>
        </div>

    </div>


  <button type="button" class="collapsible">Public AI Systems</button>
    <div class="content">

    <button type="button" class="collapsible">The Rule of Law for Artificial Intelligence in Public Administration: A System Safety Perspective</button>
    <div class="content">
      <p>
        <b>Abstract</b><br>
        This chapter proposes an analytical lens to comprehensively address the role of Artificial Intelligence (AI) applications in mediating arbitrary exercise of power in public administration and the citizen harms that result from such conduct. It provides a timely and urgent account to fill gaps in conventional Rule of Law thought. AI systems are socio-technical by nature and, therefore, differ from the text-driven social constructs that the legal professions dealing with Rule of Law issues concentrate on. Put to work in public administration contexts with consequential decision-making, technical artefacts can contribute to a variety of hazardous situations that provide opportunities for arbitrary conduct. A comprehensive lens to understand and address the role of technology in Rule of Law violations has largely been missing in literature. We propose to combine a socio-legal perspective on the Rule of Law with central insights from system safety–a safety engineering tradition with a strong scientific as well as real-world practice–that considers safety from a technological, systemic, and institutional perspective. The combination results in a lexicon and analytical approach that enables public organisations to identify possibilities for arbitrary conduct in public AI systems. Following on the analysis, interventions can be designed to prevent, mitigate, or correct system hazards and, thereby, protect citizens against arbitrary exercise of power.
        <br><br>
        Nouws, S. J. J., & Dobbe, R. I. J. (2024). The Rule of Law for Artificial Intelligence in Public Administration: A System Safety Perspective. In Digital Governance: confronting the challenges posed by Artificial Intelligence. TMC Asser Press.
      </p>
    </div>
  
    <button type="button" class="collapsible">Diagnosing and Addressing Emergent Harms in the Design Process of Public AI and Algorithmic Systems</button>
        <div class="content">
          <p>
            <b>Abstract</b><br>
            Algorithmic and data-driven systems are increasingly used in the public sector to improve the efficiency of existing services or to provide new services through the newfound capacity to process vast volumes of data. Unfortunately, certain instances also have negative consequences for citizens, in the form of discriminatory outcomes, arbitrary decisions, lack of recourse, and more. These have serious impacts on citizens ranging from material to psychological harms. These harms partly emerge from choices and interactions in the design process. Existing critical and reflective frameworks for technology design do not address several aspects that are important to the design of systems in the public sector, namely protection of citizens in the face of potential algorithmic harms, the design of institutions to ensure system safety, and an understanding of how power relations affect the design, development, and deployment of these systems. The goal of this workshop is to develop these three perspectives and take the next step towards reflective design processes within public organisations. The workshop will be divided into two parts. In the first half we will elaborate the conceptual foundations of these perspectives in a series of short talks. Workshop participants will learn new ways of protecting against algorithmic harms in sociotechnical systems through understanding what institutions can support system safety, and how power relations influence the design process. In the second half, participants will get a chance to apply these lenses by analysing a real world case, and reflect on the challenges in applying conceptual frameworks to practice.
            <br><br>
            Nouws, S., Martinez De Rituerto De Troya, Í., Dobbe, R., & Janssen, M. (2023, July). Diagnosing and Addressing Emergent Harms in the Design Process of Public AI and Algorithmic Systems. In Proceedings of the 24th Annual International Conference on Digital Government Research (pp. 679-681). <a href="https://doi.org/10.1145/3598469.3598557">https://doi.org/10.1145/3598469.3598557</a>
          </p>
        </div>

      <button type="button" class="collapsible">Dismantling digital cages: Examining design practices for public algorithmic systems</button>
        <div class="content">
          <p>
            <b>Abstract</b><br>
            Algorithmic systems used in public administration can create or reinforce <i>digital cages</i>. A digital cage refers to algorithmic systems or information architectures that create their own reality through formalization, frequently resulting in incorrect automated decisions with severe impact on citizens. Although much research has identified how algorithmic artefacts can contribute to digital cages and their unintended consequences, the emergence of digital cages from human actions and institutions is poorly understood. Embracing a broader lens on how technology, human activity, and institutions shape each other, this paper explores what design practices in public organizations can result in the emergence of digital cages. Using Orlikowski’s structurational model of technology, we found four design practices in observations and interviews conducted at a consortium of public organizations. This study shows that design processes of public algorithmic systems (1) are often narrowly focused on technical artefacts, (2) disregard the normative basis for these systems, (3) depend on involved actors’ awareness of socio-technics in public algorithmic systems, (4) and are approached as linear rather than iterative. These four practices indicate that institutions and human actions in design processes can contribute to the emergence of digital cages, but also that institutional – opposed to technical – possibilities to address their unintended consequences are often ignored. Further research is needed to examine how design processes in public organizations can evolve into socio-technical processes, can become more democratic, and how power asymmetries in the design process can be mitigated.
            <br><br>
            Nouws, S., Janssen, M., & Dobbe, R. (2022, August). Dismantling digital cages: Examining design practices for public algorithmic systems. In International Conference on Electronic Government (pp. 307-322). Cham: Springer International Publishing.<a href="https://link.springer.com/chapter/10.1007/978-3-031-15086-9_20">https://link.springer.com/chapter/10.1007/978-3-031-15086-9_20</a>
          </p>
        </div>

    </div>


  <button type="button" class="collapsible">Energy Justice</button>
    <div class="content">
  
    <button type="button" class="collapsible">A Review of Fairness Conceptualizations in Electrical Distribution Grid Congestion Management</button>
        <div class="content">
          <p>
            <b>Abstract</b><br>
            Fairness has recently gained significant attention in the scientific literature on algorithmic control systems for congestion management. However, many diverse conceptualizations of fairness have been presented. This paper aims to categorize these varying conceptualizations by reviewing existing literature on congestion management. It examines how researchers approach decisions concerning the scoping of fairness problems, the selection of fairness principles, and the choice of evaluation metrics. Findings highlight a need for more justification of fairness conceptualizations in literature as well as a need for standardized evaluation metrics and more empirical grounding and validation. The insights provided can help researchers and practitioners consider fairness comprehensively in the design of algorithmic control systems for congestion management.
            <br><br>
            de Winkel, E., Lukszo, Z., Neerincx, M., and Dobbe, R. (2024) A Review of Fairness Conceptualizations in Electrical Distribution Grid Congestion Management. Conference of Innovative Smart Grid Technologies 2024.
          </p>
        </div>

    </div>
  

<h3>Get in touch</h3>
  <p>Like this & like that.</p>

<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>

</body>
</html>
